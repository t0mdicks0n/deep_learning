{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "52a0da50-d1bb-4e1b-bd16-2d859c8db629",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "52a0da50-d1bb-4e1b-bd16-2d859c8db629",
        "outputId": "851b8f24-a0a2-4872-c125-61222b2f93c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training LSTM (Longer Run)...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'random' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1790300355.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;31m# Since we only look at 1 name at a time, we need MANY loops to see the dataset enough times.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m25000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0minput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'random' is not defined"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# --- 1. PREPARATION: The Vocabulary ---\n",
        "# We need to map every letter to a unique number.\n",
        "# We include a generic set of characters (a-z) and a few extras.\n",
        "all_letters = \"abcdefghijklmnopqrstuvwxyzåäöABCDEFGHIJKLMNOPQRSTUVWXYZÅÄÖ .,;'\"\n",
        "n_letters = len(all_letters)\n",
        "\n",
        "def name_to_tensor(name):\n",
        "    \"\"\"\n",
        "    Converts a string \"Anna\" into a Tensor of shape (Length, 1, n_letters).\n",
        "    We use 'One-Hot Encoding': A vector of zeros with a single '1' at the letter's index.\n",
        "    \"\"\"\n",
        "    tensor = torch.zeros(len(name), 1, n_letters)\n",
        "    for li, letter in enumerate(name):\n",
        "        # Find the index of the letter in our list\n",
        "        try:\n",
        "            index = all_letters.index(letter)\n",
        "            tensor[li][0][index] = 1\n",
        "        except ValueError:\n",
        "            continue # Ignore weird characters\n",
        "    return tensor\n",
        "\n",
        "# --- 2. SWEDISH DATASET ---\n",
        "# 0 = Boy (Pojke), 1 = Girl (Flicka)\n",
        "training_data = [\n",
        "    # --- POJKAR (Boys) ---\n",
        "    # The \"Names ending in E\" (Nicknames)\n",
        "    (\"Lasse\", 0), (\"Bosse\", 0), (\"Nisse\", 0), (\"Olle\", 0), (\"Hasse\", 0),\n",
        "    (\"Janne\", 0), (\"Roffe\", 0), (\"Kalle\", 0), (\"Pelle\", 0), (\"Sune\", 0),\n",
        "    (\"Åke\", 0), (\"Börje\", 0), (\"Arne\", 0), (\"Svante\", 0), (\"Christer\", 0),\n",
        "\n",
        "    # Classic Swedish\n",
        "    (\"Gunnar\", 0), (\"Anders\", 0), (\"Johan\", 0), (\"Lars\", 0), (\"Mikael\", 0),\n",
        "    (\"Per\", 0), (\"Karl\", 0), (\"Hans\", 0), (\"Peter\", 0), (\"Jan\", 0),\n",
        "    (\"Thomas\", 0), (\"Erik\", 0), (\"Fredrik\", 0), (\"Bengt\", 0), (\"Sven\", 0),\n",
        "    (\"Magnus\", 0), (\"Gustav\", 0), (\"Oscar\", 0), (\"William\", 0), (\"Lucas\", 0),\n",
        "    (\"Alexander\", 0), (\"Björn\", 0), (\"Ulf\", 0), (\"Göran\", 0), (\"Stefan\", 0),\n",
        "    (\"Mats\", 0), (\"Leif\", 0), (\"Marcus\", 0), (\"Torbjörn\", 0), (\"Kjell\", 0),\n",
        "    (\"Håkan\", 0), (\"Lennart\", 0), (\"Stig\", 0), (\"Kerstin\", 0), (\"Olof\", 0),\n",
        "    (\"Rolf\", 0), (\"Tommy\", 0), (\"Ingvar\", 0), (\"Kenneth\", 0), (\"Jörgen\", 0),\n",
        "\n",
        "    # --- FLICKOR (Girls) ---\n",
        "    # The \"Confusing E endings\"\n",
        "    (\"Marie\", 1), (\"Therese\", 1), (\"Louise\", 1), (\"Sofie\", 1), (\"Emelie\", 1),\n",
        "    (\"Natalie\", 1), (\"Alice\", 1), (\"Tove\", 1), (\"Signe\", 1), (\"Beatrice\", 1),\n",
        "    (\"Irene\", 1), (\"Caroline\", 1), (\"Madeleine\", 1), (\"Amelie\", 1), (\"Elin\", 1),\n",
        "\n",
        "    # Classic Swedish\n",
        "    (\"Anna\", 1), (\"Eva\", 1), (\"Maria\", 1), (\"Karin\", 1), (\"Kristina\", 1),\n",
        "    (\"Lena\", 1), (\"Sara\", 1), (\"Malin\", 1), (\"Emma\", 1), (\"Ingrid\", 1),\n",
        "    (\"Birgitta\", 1), (\"Marianne\", 1), (\"Jenny\", 1), (\"Maja\", 1), (\"Elsa\", 1),\n",
        "    (\"Julia\", 1), (\"Linnea\", 1), (\"Wilma\", 1), (\"Ebba\", 1), (\"Ida\", 1),\n",
        "    (\"Saga\", 1), (\"Klara\", 1), (\"Ulla\", 1), (\"Elisabeth\", 1), (\"Monica\", 1),\n",
        "    (\"Hanna\", 1), (\"Linda\", 1), (\"Susanne\", 1), (\"Agneta\", 1), (\"Katarina\", 1),\n",
        "    (\"Gunilla\", 1), (\"Annika\", 1), (\"Britt\", 1), (\"Inger\", 1), (\"Åsa\", 1),\n",
        "    (\"Siv\", 1), (\"Barbro\", 1), (\"Lisbet\", 1), (\"Maj\", 1), (\"Anita\", 1)\n",
        "]\n",
        "\n",
        "# --- 3. THE MODEL (RNN) ---\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # LSTM replaces RNN\n",
        "        # It's smarter at keeping context over long sequences\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        # LSTM needs TWO hidden states:\n",
        "        # 1. h0 (Hidden state - working memory)\n",
        "        # 2. c0 (Cell state - long-term memory)\n",
        "        h0 = torch.zeros(1, 1, self.hidden_size)\n",
        "        c0 = torch.zeros(1, 1, self.hidden_size)\n",
        "\n",
        "        # LSTM returns: output, (hidden_state, cell_state)\n",
        "        _, (hidden, cell) = self.lstm(input_tensor, (h0, c0))\n",
        "\n",
        "        # We use the final hidden state for prediction\n",
        "        output = self.fc(hidden[0])\n",
        "        output = self.sigmoid(output)\n",
        "        return output\n",
        "\n",
        "# --- 4. TRAINING ---\n",
        "# Setup\n",
        "model = LSTM(n_letters, 128, 1) # Keep Hidden size 128\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# INCREASE LEARNING RATE: 0.005 -> 0.01 (Helps get out of the 0.69 plateau)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "print(\"Training LSTM (Longer Run)...\")\n",
        "loss_avg = 0\n",
        "\n",
        "# INCREASE ITERATIONS: 2000 -> 25000\n",
        "# Since we only look at 1 name at a time, we need MANY loops to see the dataset enough times.\n",
        "for epoch in range(25000):\n",
        "    name, label = random.choice(training_data)\n",
        "\n",
        "    input_tensor = name_to_tensor(name)\n",
        "    target_tensor = torch.tensor([[float(label)]])\n",
        "\n",
        "    output = model(input_tensor)\n",
        "    loss = criterion(output, target_tensor)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    loss_avg += loss.item()\n",
        "\n",
        "    if (epoch+1) % 5000 == 0:\n",
        "        print(f\"Iteration {epoch+1}, Avg Loss: {loss_avg/5000:.4f}\")\n",
        "        loss_avg = 0\n",
        "\n",
        "print(\"Training finished!\")\n",
        "\n",
        "# --- 3. RE-TEST ---\n",
        "print(\"\\n--- TEST RESULTS ---\")\n",
        "test_names = [\"Gunnar\", \"Annika\", \"Torbjörn\", \"Åsa\", \"Kjell\", \"Olle\", \"Louise\", \"Nisse\", \"Lasse\", \"Therese\"]\n",
        "\n",
        "print(\"\\n--- THE FINAL EXAM (Names NEVER seen before) ---\")\n",
        "\n",
        "for name in test_names:\n",
        "    predict(name)\n",
        "\n",
        "# These names are NOT in your training_data list\n",
        "final_exam = [\n",
        "    \"Sixten\",   # Boy (Classic)\n",
        "    \"Ellen\",    # Girl (Classic)\n",
        "    \"Hjalmar\",  # Boy (Old school)\n",
        "    \"Ebba\",     # Girl (Very popular)\n",
        "    \"Love\",     # Boy (The ultimate trick! Ends in e, looks like English 'Love')\n",
        "    \"Signe\",    # Girl (Ends in e)\n",
        "    \"Melker\",   # Boy (Ends in r)\n",
        "    \"Astrid\",   # Girl (Ends in d)\n",
        "    \"Pontus\",   # Boy (Ends in s, but not double s)\n",
        "    \"Malte\"     # Boy (Ends in e, generic structure)\n",
        "]\n",
        "\n",
        "for name in final_exam:\n",
        "    predict(name)\n",
        "\n",
        "# --- 5. TESTING ---\n",
        "def predict(name):\n",
        "    with torch.no_grad():\n",
        "        tensor = name_to_tensor(name)\n",
        "        output = model(tensor)\n",
        "        prob = output.item()\n",
        "\n",
        "        # If > 0.5 it's a Girl (1), else Boy (0)\n",
        "        guess = \"Girl\" if prob > 0.5 else \"Boy\"\n",
        "        confidence = prob if prob > 0.5 else 1 - prob\n",
        "        print(f\"Name: {name:10} -> Prediction: {guess} ({confidence*100:.1f}%)\")\n",
        "\n",
        "print(\"\\n--- TEST RESULTS (Unseen Names) ---\")\n",
        "predict(\"Gunnar\")   # Boy (Ends in r)\n",
        "predict(\"Annika\")   # Girl (Ends in a)\n",
        "predict(\"Torbjörn\") # Boy (Ends in n)\n",
        "predict(\"Åsa\")      # Girl (Short, ends in a)\n",
        "predict(\"Kjell\")    # Boy (Double consonant)\n",
        "predict(\"Olle\")\n",
        "predict(\"Louise\")\n",
        "predict(\"Nisse\")\n",
        "predict(\"Lasse\")\n",
        "\n",
        "predict(\"Peter\")\n",
        "predict(\"Lollo\")\n",
        "predict(\"Kalle\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "51af2f26-9e72-4f00-bed7-1f440b562bb4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51af2f26-9e72-4f00-bed7-1f440b562bb4",
        "outputId": "7e0bf162-8b44-4b08-e0b4-d1918e804b69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Analyzing: Ellen\n",
            "Step 1 ('E'): Boy (50.4%)\n",
            "Step 2 ('El'): Boy (50.3%)\n",
            "Step 3 ('Ell'): Boy (50.3%)\n",
            "Step 4 ('Elle'): Boy (50.4%)\n",
            "Step 5 ('Ellen'): Boy (50.3%)\n",
            "\n",
            "Analyzing: Lollo\n",
            "Step 1 ('L'): Boy (50.1%)\n",
            "Step 2 ('Lo'): Boy (50.3%)\n",
            "Step 3 ('Lol'): Boy (50.2%)\n",
            "Step 4 ('Loll'): Boy (50.2%)\n",
            "Step 5 ('Lollo'): Boy (50.3%)\n"
          ]
        }
      ],
      "source": [
        "def analyze_name(name):\n",
        "    print(f\"\\nAnalyzing: {name}\")\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        input_tensor = name_to_tensor(name)\n",
        "\n",
        "        # We need to manually step through the LSTM to see the hidden state evolve\n",
        "        h0 = torch.zeros(1, 1, 128)\n",
        "        c0 = torch.zeros(1, 1, 128)\n",
        "        hidden = (h0, c0)\n",
        "\n",
        "        for i in range(len(name)):\n",
        "            # Feed one letter at a time\n",
        "            letter_tensor = input_tensor[i].unsqueeze(0)\n",
        "            _, hidden = model.lstm(letter_tensor, hidden)\n",
        "\n",
        "            # Predict based on current memory\n",
        "            output = model.fc(hidden[0])\n",
        "            prob = model.sigmoid(output).item()\n",
        "\n",
        "            # Print the brain's status\n",
        "            guess = \"Girl\" if prob > 0.5 else \"Boy\"\n",
        "            conf = prob if prob > 0.5 else 1 - prob\n",
        "            print(f\"Step {i+1} ('{name[:i+1]}'): {guess} ({conf*100:.1f}%)\")\n",
        "\n",
        "# Run it\n",
        "analyze_name(\"Ellen\")\n",
        "analyze_name(\"Lollo\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}